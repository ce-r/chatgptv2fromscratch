# -*- coding: utf-8 -*-
"""chatgpt_module3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HNTFyAQuVh9ImOGGgm427wPdHkguaM_H
"""

# tokenizer.py

'''
This class should be constructed with trained tokenizer data:
vocab_file : a string path to a vocab.txt file
merges_file : a string path to a merges.json file

The class should implement two methods:
encode(string): returns a list of integer ids (tokenized text)
decode(list_of_ids): returns a string re-assembled from token ids

A good sanity check is that decode(encode(x)) should return x.

You may assume that only a single sample is passed in at a time (no batching).
You can add additional methods, classes, etc as you find helpful.

Important: Unlike Assignment 2, our vocabulary and merges may include
punctuation. Just treat all non-space characters equally.
'''

import json

class Tokenizer:
  # main idea: encode/decode predefined tokens
  # given by the merges files, define new tokens
  def __init__(self, vocab_file, merges_file):
    # TODO
    # load vocab and merges files
    with open(vocab_file, 'r', encoding='utf-8') as vf:
      # extract the index and word/token for k,v -> word/token, index
      self.vocab = {line.strip(): idx for idx, line in enumerate(vf)}

    with open(merges_file, 'r', encoding='utf-8') as mf:
      merges = json.load(mf)

    # map merges list to tuple, integer or k,v dict,
    # rank tuple keys by integer value for fast hashing/encoding
    # lowest rank has the highest priority
    # relating to max freq order in merges file
    self.bpe_priority = {tuple(merge): idx for idx, merge in enumerate(merges)}
    # define unknown_id in case of unrecognized chars/tokens
    self.unknown = self.vocab.get("[*69]", -1)
    # for decoding, k,v -> index, token
    self.id_to_token = {idx: token for token, idx in self.vocab.items()}

    # fast lookup for frequently tokenized sequences,
    # improves time complexity of encode()
    self.cache = {}

  def get_pairs(self, tokens): # tokens list to set of tuples
    # get adjacent values and create tuple pairs for the BPE algo
    return {(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)}

  def encode(self, string):
    '''
    param string : a string to be encoded
    returns a list of integers (token ids)
    '''

    # encode() uses BPE to tokenize string
    # sequences and caches results for efficiency

    # TODO
    # first check cache if sequence has already been tokenized
    # use cache to avoid redundant tokenization
    if string in self.cache:
      return self.cache[string]

    # atomize sequence into chars
    tokens = list(string)
    # begin bpe process by generating pairs for while loop
    pairs = self.get_pairs(tokens)
    # print(f"pairs: {pairs}")
    while pairs:           # filter set of tuples [pairs] using merges set [bpe_priority]
                           # iteratively merge most freq token pairs
      priority_merge = min((pair for pair in pairs if pair in self.bpe_priority),
                           key=lambda pair: self.bpe_priority[pair], # min by v in k,v
                           default=None) # GET min idx, highest priority (freq) key,
      # print(f"priority merge: {priority_merge}")
      if not priority_merge: # if pairs still contains a tuple in bpe_priority
                             # continue main loop assembling tokens, building
                             # and encoding vocab
        # print("no priority merge")
        break

      new_tokens = [] # for adding new tokens
      i = 0
      a, b = priority_merge # unpack tuple
      while i < len(tokens): # proceed to merge values that match
                             # b/w bpe_priority and adj elmts in tokens[]
        if i < len(tokens) - 1 and tokens[i] == a and tokens[i+1] == b:
          # remerge of tuple string elements
          added_token = a + b
          new_tokens.append(added_token) # add token

          if added_token not in self.vocab: # new token gets id and spot in vocab dict
            new_id = len(self.vocab) # new tokens will be encoded
                                     # as the length of the vocab
            self.vocab[added_token] = new_id # add new token to vocab dict
                                             # encoding structure
            self.id_to_token[new_id] = added_token # add token and id to decoding dictionary
          i += 2 # iterate by two if match
        else:
          new_tokens.append(tokens[i]) # add to new_tokens/tokens for later tokenization
          i += 1 # iterate by one if not a match

      tokens = new_tokens # redefine tokens based on new acquired tokens
      pairs = self.get_pairs(tokens) # generate new pairs for next check on while loop
      # print(f"after merging {priority_merge}: {tokens}")

    token_ids = [self.vocab.get(token, self.unknown) for token in tokens] # list of TOKEN IDs
    # print(f"final token IDs: {token_ids}")
    self.cache[string] = token_ids # store full sequence (k) and list of
                                   # token ids (v) cached in a dictionary
                                   # since vocab dict has small tokens covered
    return token_ids

  def decode(self, list_of_integers):
    '''
    param list_of_integers : a list of token ids
    returns a string formed by decoding these ids.
    '''

    # TODO
    # decode using the encoded list of integers as input and the
    # dictionary id_to_token holding the predefined decoding by accessing
    # the keys (integers) with the encoded integers to yield the original sequence
    tokens = [self.id_to_token.get(idx, f"<unknown:{idx}>") for idx in list_of_integers]
    # print(f"decoded tokens: {tokens}")
    return ''.join(tokens)

# embedding.py

'''
Complete this class by instantiating a parameter called "self.weight", and
use it to complete the forward() method. You do not need to worry about backpropogation.
'''

import torch

class CustomEmbedding(torch.nn.Module):
  def __init__(self, num_embeddings, embedding_dim):
    super().__init__()
    # TODO
    # randomly initialized parameters in embedding layer of size NxD
    self.weight = torch.nn.Parameter(torch.randn(num_embeddings, embedding_dim)) # N tokens, D embeddings -> (num_embeddings, embedding_dim)
    print(self.weight.size())

  def forward(self, x):
    # x is a tensor of integers
    # TODO
    # input to the weight tensor is a tokenized sequence of ids
    embeddings = self.weight[x]
    return embeddings

if __name__ == "__main__":

  # example of using this class
  tok = Tokenizer("./vocab.txt", "./merges.json")
  # print(len(tok.vocab))
  x = tok.encode("Peter piper picked a peck of pickled peppers.")
  # print(x)
  y = tok.decode(x)
  print(y) # should be our original text.

  # embed = CustomEmbedding(N, D)

  # generate embedding tensor, instance from the custom
  # embedding class with tokenized sequence ids, input x
  embed = CustomEmbedding(len(tok.vocab), 512) # arbitrarily chosen size for embedding_dim
  # print(embed(torch.tensor(x)).size())

  # print the embedding tensor
  print(embed(torch.tensor(x)))

