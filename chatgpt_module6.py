# -*- coding: utf-8 -*-
"""chatgpt_module6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ln1MOtL2KzhE2QEUz1_tjYppQ0TIKbMI
"""

!pip install datasets

from datasets import load_dataset

# ds = load_dataset("ubaada/booksum-complete-cleaned", "chapters")["train"]
ds = load_dataset("wikitext", "wikitext-103-v1")["train"]

lines = []
for entry in ds:
	text = entry["text"]
	text = text.replace("\n", " ") # remove newline formatting
	text = " ".join(text.split()) # remove sequences of whitespace
	lines.append(text+"\n")

	# if you want to work with less data, this is one place to add that.
	if len(lines) >= 100:
		break

f = open("data.txt", "w")
f.writelines(lines)
f.close()

from transformers import AutoTokenizer

'''
For module 6 onward we are using huggingface tokenizers in the interest of speed.
These are heavily optimized to tokenize large amounts of text quickly.

This tokenizer is similar to the tokenizer we trained in module 2, except:
- Uses Ä  to denote space (if you look in the outputs from saving)
- Byte-level BPE
- does a better job of preserving whitespace sequences

'''

class HFTokenizer():

	def __init__(self):
		self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
		self.tokenizer.eos_token = "<|endoftext|>"

	def train(self, datafile):
		self.tokenizer = self.tokenizer.train_new_from_iterator(
			open(datafile, "r").readlines(),
			10000,
			limit_alphabet=500,
		)
		self.tokenizer.save_pretrained("./hftokenizer/")

	def load(self):
		self.tokenizer = AutoTokenizer.from_pretrained("./hftokenizer/")
		# self.tokenizer = AutoTokenizer.from_pretrained("/home/ted/EP/gptep/modules/module6/hftokenizer/")

	def encode(self, string):
		return self.tokenizer(string)["input_ids"]

	def decode(self, list_of_ids):
		return self.tokenizer.decode(list_of_ids)

if __name__ == "__main__":

	tokenizer = HFTokenizer()
	tokenizer.train("./data.txt")
	tokenizer.load()

	x = "I want to go eat ice   <eos>cream<eos> yes."
	y = tokenizer.encode(x)
	x2 = tokenizer.decode(y)

	print(x)
	print(y)
	print(x2)

# import torch
from tqdm import tqdm
import numpy as np
# from hftokenizer import HFTokenizer

def construct_dataset(data_txt_file, sequence_length=256):
  '''
  data_txt_file : a string path to a text file containing training data, one sample per line
  sequence_length : int, the desired length of each training sequence

  This method should use the trained tokenizer to convert samples to token_ids, and
  then pack them into a training set represented as a 2D array of size (sequences, sequence_length+1).
  The +1 is very important! It lets us compare our model outputs to the sequence shifted by one.

  You can save this training set in whatever format you wish for loading into the training script.
  I recommend using numpy's np.save() method or the pickle module.

  The saved data should be shuffled so we can directly load it and train on it in the training script.
  '''

  # construct tokenizer
  tokenizer = HFTokenizer()
  tokenizer.load()

  # get all samples
  f = open(data_txt_file, "r")
  samples = f.readlines()
  samples = [x.replace("\n", "") for x in samples]

  # ----------------------------------------
  # TODO
  # - add '<|endoftext|>' to each sample or add tokenizer.eos_token_id after tokenizing.
  # - use tokenizer.encode() to tokenize each sample
  # - pack into sequences of length sequence_length
  # - shuffle
  # - save out data

  tokenized_samples = []
  for sample in samples:
    if len(sample) == 0:
      continue
    tokens = tokenizer.encode(sample) # tokenize sample with AutoTokenizer
    # print(tokens)
    tokens.append(tokenizer.tokenizer.eos_token_id) # append eos at end of each sample
    tokenized_samples.extend(tokens) # one long list of token ids

  num_sequences = len(tokenized_samples) // (sequence_length + 1) # number of arrays/rows each w/ same number of tokens/columns
  sequences = np.array(tokenized_samples[:num_sequences * (sequence_length + 1)]).reshape(num_sequences, sequence_length + 1)
  np.random.shuffle(sequences)
  # print(sequences.shape)
  np.save("./data.npy", sequences)

if __name__ == "__main__":
  construct_dataset("./data.txt", 256)

import torch
import math

class CustomLinear(torch.nn.Module):

	def __init__(self, input_size, output_size):
		super().__init__()
		self.weight = torch.nn.Parameter(0.01*torch.randn((output_size, input_size)))
		self.bias = torch.nn.Parameter(torch.zeros((output_size,)))

	def forward(self, x):
		return x @ self.weight.T + self.bias


class CustomEmbedding(torch.nn.Module):

	def __init__(self, num_embeddings, embedding_dim):
		super().__init__()
		self.weight = torch.nn.Parameter(0.01*torch.randn((num_embeddings, embedding_dim)))

	def forward(self, x):
		# if x.max() >= self.weight.shape[0] or x.min() < 0:
		# 	print(f"invalid token idx: max {x.max()}, min {x.min()}")
		return self.weight[x]


class CustomMHA(torch.nn.Module):

	def __init__(self, d_model, n_heads):
		super().__init__()
		self.d_model = d_model
		self.n_heads = n_heads
		self.qkv = torch.nn.Parameter(0.01*torch.randn((3*d_model, d_model)))
		self.wo = torch.nn.Parameter(0.01*torch.randn((d_model, d_model)))

	def forward(self, x):
		added_batch = False
		if len(x.shape) == 2:
			added_batch = True
			x = x[None,:,:]

		# queries, keys, and values
		B, S, D = x.shape
		QKV = x @ self.qkv.T # B, S, 3D
		Q, K, V = torch.chunk(QKV, 3, -1)

		# split into multiple heads
		dh = D//self.n_heads
		q_heads = torch.reshape(Q, (B, S, self.n_heads, dh))
		k_heads = torch.reshape(K, (B, S, self.n_heads, dh))
		v_heads = torch.reshape(V, (B, S, self.n_heads, dh))

		# reshape into (B*h, S, dh) so we isolate sequences for each head
		q_heads = torch.transpose(q_heads, 1, 2).reshape((B*self.n_heads, S, dh))
		k_heads = torch.transpose(k_heads, 1, 2).reshape((B*self.n_heads, S, dh))
		v_heads = torch.transpose(v_heads, 1, 2).reshape((B*self.n_heads, S, dh))

		# make attention mask
		mask = torch.ones((S,S))
		mask = torch.tril(mask)
		mask = mask[None, :, :]
		mask = mask.to(x.device)

		# attention
		k_heads_t = torch.transpose(k_heads, 1, 2)
		qkt = torch.matmul(q_heads, k_heads_t) / math.sqrt(float(dh))
		qkt = qkt*mask
		qkt[qkt==0] = float('-inf')
		attn = torch.nn.functional.softmax(qkt, dim=-1)
		x = torch.matmul(attn, v_heads)

		# shmush back into the correct shape
		x = torch.reshape(x, (B, self.n_heads, S, dh))
		x = torch.transpose(x, 1, 2) # B, S, h, dh
		x = torch.reshape(x, (B, S, D))

		# apply projection
		x = x @ self.wo.T

		if added_batch:
			x = x[0]

		return x


class TransformerDecoderBlock(torch.nn.Module):

	def __init__(self, d_model, n_heads):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm((d_model,))
		self.mha = CustomMHA(d_model, n_heads)
		self.norm2 = torch.nn.LayerNorm((d_model,))
		self.fc1 = CustomLinear(d_model, 4*d_model)
		self.act = torch.nn.ReLU()
		self.fc2 = CustomLinear(4*d_model, d_model)
		self.dropout = torch.nn.Dropout(0.1)

	def forward(self, x):
		x = x + self.mha(self.norm1(x))
		x = x + self.dropout(self.fc2(self.act(self.fc1(self.norm2(x)))))
		return x


class GPTModel(torch.nn.Module):

	def __init__(self, d_model, n_heads, layers, vocab_size, max_seq_len):
		super().__init__()

		self.word_embeddings = CustomEmbedding(vocab_size, d_model)
		self.position_embeddings = CustomEmbedding(max_seq_len, d_model)

		self.layers = torch.nn.ModuleList()
		for i in range(layers):
			block = TransformerDecoderBlock(d_model, n_heads)
			self.layers.append(block)

		self.fc_out = CustomLinear(d_model, vocab_size)

	def forward(self, x):
		B, S = x.shape
		# positions = torch.arange(S).to(torch.long).to(x.device)
		positions = torch.arange(S, device=x.device).clamp(0, 255).to(torch.long) # make sure pos ids within range max_seq_len=256
		positions = positions[None, :]
		positions = positions.repeat(B, 1)

		# print(f"max token id in batch: {x.max().item()}, min token id: {x.min().item()}")
		# print(f"max position id in batch: {positions.max().item()}, min position id: {positions.min().item()}")

		w_emb = self.word_embeddings(x)
		p_emb = self.position_embeddings(positions)

		# print(w_emb.shape) # (B, S, D)
		# print(p_emb.shape) # (B, S, D)
		# print(f"w_emb nans: {torch.isnan(w_emb).sum()}")
		# print(f"p_emb nans: {torch.isnan(p_emb).sum()}")

		x = w_emb + p_emb

		for layer in self.layers:
			x = layer(x)

		logits = self.fc_out(x)
		device = torch.device("cuda")
		logits = logits.to(device)

		return logits

if __name__ == "__main__":
  model = GPTModel(128, 8, 4, 1000, 512)
  B = 32
  S = 48
  x = torch.randint(1000, (B, S))
  y = model(x)
  # print(y)

"""# choose batch of sequences. not tokens this time.
# torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
"""

# import torch
# import numpy as np
# from gpt import GPTModel
import matplotlib.pyplot as plt
import os

# since we didn't really cover how to do this in lecture-
# this creates a learning rate schedule for you. Refer to the
# pytorch docs for more info on using a scheduler.

# This one is designed for you to call scheduler.step() on every
# model update step.
def cosine_with_warmup_lr_scheduler(opt, total_steps, warmup_steps):
  def thunk(stepnum):
    if stepnum <= warmup_steps:
      # go from ~0 to 1.0
      prog = float(stepnum)/float(warmup_steps)
      lrmult = 0.00001 + prog
    else:
      # go from 1.0 to ~0
      steps_after_peak = stepnum-warmup_steps
      tail_steps = total_steps-warmup_steps
      prog = float(steps_after_peak) / float(tail_steps)
      lrmult = ((np.cos(3.141592*prog)+1.0)*0.5)*0.9 + 0.1
    return max(lrmult, 0.1)
  scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=thunk)
  return scheduler

# ===========================================================================

'''
Complete the following method which trains a GPT model and saves a loss curve.

To reiterate: you don't need to worry about weight decay, weight initialization, grad accumulation, or weight tying.
Use whatever batch size you are able, even something like 2 or 4 is fine.
Use a few hundred warmup steps and a peak learning rate that is (something x 10-4).
'''
def train():
  # pytorch will report cuda errors for debugging immediately
  os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

  device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # use "cpu" if not gpu available

  # adjust as needed
  model = GPTModel(d_model=512, n_heads=16, layers=8, vocab_size=10000, max_seq_len=256)
  param_count = sum(p.numel() for p in model.parameters())
  print("Model has", param_count, "parameters.")

  model = model.to(device) # (B, S, L)

  '''
  # TODO

  pseudocode:
  opt = torch.optim.AdamW(...
  scheduler = cosine_with_warmup_lr_scheduler(...

  for batch in dataset:
      opt.zero_grad()

      <run batch through model and compute loss>

      # VERY IMPORTANT ---
      torch.nn.CrossEntropyLoss expects classes in the 2nd dimension.
      You may need to transpose dimensions around to make this work.
      It also expects unnormalized logits (no softmax).
      # ---

      loss.backward()

      # clip the gradient
      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

      # step the optimizer and scheduler
      opt.step()
      scheduler.step()

      # log total tokens and loss
      # periodically save a plot of loss vs tokens

  '''
  # optimizer
  opt = torch.optim.AdamW(model.parameters(), lr=0.0001, betas=(0.9, 0.95))
  scheduler = cosine_with_warmup_lr_scheduler(opt, 10000, 1000)

  construct_dataset("./data.txt", 256)
  dataset = np.load("./data.npy", allow_pickle=True)
  dataset = torch.tensor(dataset, dtype=torch.long)
  dataset = dataset.to(device)

  # IMPORTANT! check if all params require grads and fix missing grads
  for name, param in model.named_parameters():
    print(f" {name}: requires_grad = {param.requires_grad}")
    if param.requires_grad is None:
      # print(f"warning: {name} has no grads, setting requies_grad=True")
      param.requires_grad = True # make sure all layers active in backprop

  losses = []
  tokens_processed = []
  total_tokens = 0

  # dataset would contain one batch of size (257, )
  # must use mini batches
  minibatch_size = 4
  num_minibatches = dataset.shape[0] // minibatch_size

  # automatic mixed precision for fast training
  scaler = torch.cuda.amp.GradScaler()

  # B in (B, S, V) and (B, S) is batch size or number of sequences in each batch
  for i in range(num_minibatches):

    # IMPORTANT! check if all model params have valid grads
    for param in model.parameters():
      if param.grad is None:
        param.grad = torch.zeros_like(param) # prevent None grads error

    opt.zero_grad(set_to_none=False) # reset grads before next minibatch to
                                     # prevent accumulation of stale grads

    minibatch = dataset[i*minibatch_size : (i+1)*minibatch_size].to(device) # (B, S), mini batching
    # print(batch.shape)

    # with torch.cuda.amp.autocast():
    #   logits = model(minibatch)
    #   logits = logits.transpose(1,2)
    #   logits = logits[:, : , :-1].contiguous()
    #   targets = minibatch[:, 1:].contiguous()
    #   loss = torch.nn.functional.cross_entropy(logits, targets)

    logits = model(minibatch) # (B, S, V) tensor
    logits = logits.transpose(1,2) # (B, V, S), dim for cross_entropy()
    logits = logits[:, : , :-1].contiguous() # (B, V, S-1) remove target

    targets = minibatch[:, 1:].contiguous() # (B, S-1)

    # print(f"logits shape: {logits.shape}, targets shape: {targets.shape}")
    # print(f"logits max: {logits.max().item()}, min: {logits.min().item()}")
    # print(f"targets max: {targets.max().item()}, min: {targets.min().item()}")
    # print(f"logits requires_grad: {logits.requires_grad}")

    loss = torch.nn.functional.cross_entropy(logits, targets)
    print(f"loss: {loss.item()}")

    try:
      loss.backward()
      print("loss.backward() executed successfully")
    except Exception as e:
      print(f"loss.backward() failed: {e}")

    # check for nans in grads
    # for name, param in model.named_parameters():
    #   print(f" {name}: requires_grad = {param.requires_grad}")
    #   if param.requires_grad is not None:
    #     print(f"grad nans in {name}: {torch.isnan(param.grad).sum().item()}")
    #   else:
    #     print(f"warning: {name} has no grads, param.grad is None")

    # "regularizes" grads to prevent explosion
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    opt.step()
    scheduler.step()

    minibatch_size, seq_len = minibatch.shape
    total_tokens += minibatch_size * (seq_len - 1)

    losses.append(loss.item())
    tokens_processed.append(total_tokens)

    # print(f"Step: {len(losses)}, Loss: {loss.items()}, Tokens processed: {total_tokens}")

  # save model weights if you want
  torch.save(model.state_dict(), "./model_weights.pt")

  plt.figure(figsize=(10, 6))
  plt.plot(tokens_processed, losses, label="Loss")
  plt.xlabel("Tokens Processed")
  plt.ylabel("Loss")
  plt.title("Loss vs Tokens Processed")
  plt.legend()
  plt.grid()
  plt.savefig("lossandtokens.png")
  plt.show()

if __name__ == "__main__":
  train()

