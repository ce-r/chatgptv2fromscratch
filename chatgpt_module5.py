# -*- coding: utf-8 -*-
"""chatgpt_module5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q3hz5qhPXULms8Uz8CloFFPDxaBImYNX
"""

import torch
import numpy as np

# linear.py

class CustomLinear(torch.nn.Module):

	def __init__(self, input_size, output_size):
		super().__init__()
		self.weight = torch.nn.Parameter(0.01*torch.randn((output_size, input_size)))
		self.bias = torch.nn.Parameter(torch.zeros((output_size,)))

	def forward(self, x):
		return x @ self.weight.T + self.bias

# embedding.py

class CustomEmbedding(torch.nn.Module):

	def __init__(self, num_embeddings, embedding_dim):
		super().__init__()
		self.weight = torch.nn.Parameter(0.01*torch.randn((num_embeddings, embedding_dim)))

	def forward(self, x):
		return self.weight[x]

# mha.py

import math

'''
Complete this module such that it includes causal masking.
There are two TODOs to fill in below:
(1) the creation of the mask and (2) the application of the mask.
After applying the mask you proceed with softmax as usual.
'''

class CustomMHA(torch.nn.Module):

	def __init__(self, d_model, n_heads):
		super().__init__()
		self.d_model = d_model
		self.n_heads = n_heads
		self.W_qkv = torch.nn.Parameter(0.01*torch.randn((3*d_model, d_model)))
		self.W_o = torch.nn.Parameter(0.01*torch.randn((d_model, d_model)))

	def causal_attn_mask(self, qkt):
		mask = torch.tril(torch.ones_like(qkt)) # lower triangular ones, zeros above
		ca_mask = torch.where(mask==1, torch.tensor(0.0), torch.tensor(float('-inf')))
		# invert and replace upper by -inf
		return ca_mask

	def forward(self, x):
		added_batch = False
		if len(x.shape) == 2:
			added_batch = True
			x = x[None,:,:]

		# queries, keys, and values
		B, S, D = x.shape
		QKV = x @ self.W_qkv.T # B, S, 3D
		Q, K, V = torch.chunk(QKV, 3, -1)

		# split into multiple heads
		dh = D//self.n_heads
		q_heads = torch.reshape(Q, (B, S, self.n_heads, dh))
		k_heads = torch.reshape(K, (B, S, self.n_heads, dh))
		v_heads = torch.reshape(V, (B, S, self.n_heads, dh))

		# reshape into (B*h, S, dh) so we isolate sequences for each head
		# you could also keep in four dimensions if you want (B,h,S,dh).
		# We are computing B*H independent attentions in parallel.
		q_heads = torch.transpose(q_heads, 1, 2).reshape((B*self.n_heads, S, dh))
		k_heads = torch.transpose(k_heads, 1, 2).reshape((B*self.n_heads, S, dh))
		v_heads = torch.transpose(v_heads, 1, 2).reshape((B*self.n_heads, S, dh))

		# compute QK^T / sqrt(d)
		k_heads_t = torch.transpose(k_heads, 1, 2)
		qkt = torch.matmul(q_heads, k_heads_t) # (B, h, S, S)
		qkt = qkt / math.sqrt(float(dh))

		# --------------------------------------
		# make causal attention mask
		# TODO
		# --------------------------------------
		ca_mask = self.causal_attn_mask(qkt)

		# --------------------------------------
		# apply causal mask
		# TODO
		# Note: Make sure to fill with -inf, not 0
		# --------------------------------------
		qkt = qkt + ca_mask

		# the rest of the attention equation
		attn = torch.nn.functional.softmax(qkt, dim=-1)
		x = torch.matmul(attn, v_heads)

		# shmush back into the correct shape
		x = torch.reshape(x, (B, self.n_heads, S, dh))
		x = torch.transpose(x, 1, 2) # B, S, h, dh
		x = torch.reshape(x, (B, S, D))

		# apply projection
		x = x @ self.W_o.T

		if added_batch:
			x = x[0]

		return x

if __name__ == "__main__":

	# same test case as last week, but should be a
	# different output now that we have added causal mask

	D = 6
	H = 2
	mha = CustomMHA(D,H) # 8 d_model, 2 heads

	# make some fixed weights
	tensor1 = torch.tensor(np.reshape(np.linspace(-2.0, 1.5, D*D*3), (D*3,D))).to(torch.float32)
	tensor2 = torch.tensor(np.reshape(np.linspace(-1.0, 2.0, D*D), (D,D))).to(torch.float32)

	# copy these into our MHA weights
	mha.W_qkv.data = tensor1
	mha.W_o.data = tensor2

	# make an input tensor
	B = 2
	S = 3
	x = torch.tensor(np.reshape(np.linspace(-1.0, 0.5, B*S*D), (B,S,D))).to(torch.float32)

	# run
	y1 = mha(x)
	print(y1.shape)
	print(y1)

	'''
	Should print out:

	torch.Size([2, 3, 6])
	tensor([[[ 21.7331,   6.4755,  -8.7821, -24.0397, -39.2973, -54.5549],
         [ 19.6692,   6.0497,  -7.5698, -21.1893, -34.8087, -48.4282],
         [ 17.6900,   5.6398,  -6.4105, -18.4607, -30.5110, -42.5612]],

        [[  2.8558,   0.8462,  -1.1635,  -3.1731,  -5.1827,  -7.1924],
         [ -1.2608,  -0.5160,   0.2287,   0.9735,   1.7182,   2.4630],
         [ -5.6875,  -2.0716,   1.5444,   5.1603,   8.7762,  12.3922]]],
       grad_fn=<UnsafeViewBackward0>)
	'''

"""# Note
##More recent architectures, rather than:
##linear(act(linear(x)))
##for the MLP, often people use the more exotic:
##linear( act(linear(x)) * linear(x) )
##See notes here: https://github.com/meta-llama/llama/issues/245
##Donâ€™t worry about this for the homework! Just use linear(act(linear(x))).
"""

# gpt.py

'''
Complete this module which handles a single "block" of our model
as described in our lecture. You should have two sections with
residual connections around them:

1) norm1, mha
2) norm2, a two-layer MLP, dropout

It is perfectly fine to use pytorch implementations of layer norm and dropout,
as well as activation functions (torch.nn.LayerNorm, torch.nn.Dropout, torch.nn.ReLU).

For layer norm, you just need to pass in D-model: self.norm1 = torch.nn.LayerNorm((d_model,))

'''
class TransformerDecoderBlock(torch.nn.Module):

  def __init__(self, d_model, n_heads):
    super().__init__()
    # TODO
    self.norm1 = torch.nn.LayerNorm((d_model,))
    self.norm2 = torch.nn.LayerNorm((d_model,))
    self.mha = CustomMHA(d_model, n_heads)
    self.linear1 = CustomLinear(d_model, 4*d_model)
    self.linear2 = CustomLinear(4*d_model, d_model)
    self.activation = torch.nn.ReLU()
    self.dropout = torch.nn.Dropout(0.1)

  def forward(self, x):
    '''
    param x : (tensor) a tensor of size (batch_size, sequence_length, d_model)
    returns the computed output of the block with the same size.
    '''
    # TODO
    n1 = self.norm1(x)
    mha_norm1 = self.mha(n1)
    resconn1 = mha_norm1 + x # 1st res connection
    mha_norm2 = self.norm2(resconn1)
    lin1 = self.linear1(mha_norm2)
    activ = self.activation(lin1)
    lin2 = self.linear2(activ)
    drop_lin2 = self.dropout(lin2)
    result = drop_lin2 + resconn1 # 2nd res connection

    return result

'''
Create a full GPT model which has two embeddings (token and position),
and then has a series of transformer block instances (layers). Finally,
the last layer should project outputs to size [vocab_size].
'''
class GPTModel(torch.nn.Module):

  def __init__(self, d_model, n_heads, layers, vocab_size, max_seq_len):
    '''
    param d_model : (int) the size of embedding vectors and throughout the model
    param n_heads : (int) the number of attention heads, evenly divides d_model
    param layers : (int) the number of transformer decoder blocks
    param vocab_size : (int) the final output vector size
    param max_seq_len : (int) the longest sequence the model can process.

    This is used to create the position embedding- i.e. the highest possible
    position to embed is max_seq_len
    '''

    super().__init__()
    # TODO
    # hint: for a stack of N layers look at torch ModuleList or torch Sequential

    self.token_embedding = CustomEmbedding(vocab_size, d_model)
    self.position_embedding = torch.nn.Embedding(max_seq_len, d_model)

    self.decoder_blocks = torch.nn.Sequential(*[TransformerDecoderBlock(d_model, n_heads) for _ in range(layers)])
    self.final_layer = CustomLinear(d_model, vocab_size)

  def forward(self, x):
    '''
    param x : (long tensor) an input of size (batch_size, sequence_length)
    which is filled with token ids

    returns a tensor of size (batch_size, sequence_length, vocab_size),
    the raw logits for the output
    '''
    # TODO
    # hint: x contains token ids, but you will also need to build a tensor of position ids here

    batch_size, seq_len = x.shape
    pos_ids = torch.arange(0, seq_len, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)
    x = self.token_embedding(x) + self.position_embedding(pos_ids)
    tr_output = self.decoder_blocks(x)
    logits = self.final_layer(tr_output)

    return logits

if __name__ == "__main__":

  # example of building the model and doing a forward pass
  D = 128
  H = 8
  L = 4
  model = GPTModel(D, H, L, 1000, 512)
  B = 32
  S = 48 # this can be less than 512, it just cant be more than 512
  x = torch.randint(1000, (B, S))
  y = model(x) # this should give us logits over the vocab for all positions

  # should be size (B, S, 1000)
  print(y.shape)
  print(y)

