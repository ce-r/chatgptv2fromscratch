# -*- coding: utf-8 -*-
"""chatgpt_module4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ucK5dKl0mSPlevBh9NyG7SvWl2Ey-CL
"""

import torch
import math
import torch.nn.functional as f

'''
Complete this module such that it computes queries, keys, and values,
computes attention, and passes through a final linear operation W_o.

You do NOT need to apply a causal mask (we will do that next week).
If you don't know what that is, don't worry, we will cover it next lecture.

Be careful with your tensor shapes! Print them out and try feeding data
through your model. Make sure it behaves as you would expect.
'''
class CustomMHA(torch.nn.Module):

	'''
	param d_model : (int) the length of vectors used in this model
	param n_heads : (int) the number of attention heads. You can
  assume that this even divides d_model.
	'''
	def __init__(self, d_model, n_heads):
		super().__init__()
		# TODO

		# ensures n_heads compatible with
		# d_model and params correctly estimated
		# 128 % 8 == 0
		assert d_model % n_heads == 0

		# please name your parameters "self.W_qkv" and "self.W_o" to aid in grading
		# self.W_qkv should have shape (3D, D)
		self.d_model = d_model
		self.n_heads = n_heads
		self.W_qkv = torch.nn.Parameter(torch.randn(3*self.d_model, self.d_model))

		# self.W_o should have shape (D,D)
		self.W_o = torch.nn.Parameter(torch.randn(self.d_model, self.d_model))

	'''
	param x : (tensor) an input batch, with size (batch_size, sequence_length, d_model)
	returns : a tensor of the same size, which has had MHA computed for each batch entry.
	'''
	def forward(self, x):
		# TODO

		# SUGGESTED OUTLINE:
		#----------

		# (1) use W_qkv to get queries Q, keys K, values V, each of shape (B,S,D)
		B, S, D = x.shape # extract batch size, seq length, embedding vector length
		qkv = torch.matmul(x, self.W_qkv.T) # shape (B, S, 3*D) / self.W_qkv.T shape (D, 3*D)
																				# for matmul, first dims B&S treated as batch dim
																				# x treated as batch of mats w/ shape (B*S, D)
																				# resulting shape (B, S, 3*D)
		Q, K, V = torch.split(qkv, D, dim=-1) # split qkv tensor of shape (B, S, 3*D) along last dim
																				  # into 3 tensors of shape (B, S, D)
																				  # q,k,v each w/ shape (B, S, D)

		# i.e.
		# (B, S, D) | (2, 3, 24) -> (B, S, h, D//h) | (2, 3, 12, 2) -> (B, h, S, D/h) | (2, 12, 3, 2)
		# D//self.n_heads will be integer and not float
		# (2) reshape these into size (B, h, S, D/h), i.e. potential heads h = (8, 12, 16, 20, 96)

		Q = Q.view(B, S, self.n_heads, D//self.n_heads).transpose(1,2)
		K = K.view(B, S, self.n_heads, D//self.n_heads).transpose(1,2)
		V = V.view(B, S, self.n_heads, D//self.n_heads).transpose(1,2)

		# (3) compute QK^T and divide by sqrt(D/h)
		# matrix multiply (B, h, S, D/h) and (B, h, D/h, S)
		attn_scores = torch.matmul(Q, K.transpose(-2,-1)) # resulting shape: (B, h, S, S)

		# normalization to keep the variance of the attention scores constant
		normalizing_factor = math.sqrt(D // self.n_heads)
		attn_scores = attn_scores / normalizing_factor

		# (4) apply softmax
		attn_weights = f.softmax(attn_scores, dim=-1)

		# (5) matrix multiply against values
		attn_output = torch.matmul(attn_weights, V) # resulting shape: (B, h, S, D/h)

		# (6) reshape (B,h,S,D/h) into (B,S,D)
		attn_output = attn_output.transpose(1,2).reshape(B,S,D)
		# (B,h,S,D/h) -> (B,S,h,D/h) -> (B,S,D)

		# (7) matrix multiply against output projection W_o
		attn_output = torch.matmul(attn_output, self.W_o.T) # resulting shape: (B, S, D)

		# (8) return
		return attn_output

		#---------

if __name__ == "__main__":

	# example of building and running this class
	mha = CustomMHA(128,8)

	# 32 samples of length 6 each, with d_model at 128 (B=32, S=6, D=128)
	x = torch.randn((32,6,128))
	y = mha(x)
	print(x.shape, y.shape) # should be the same

# this is a test case to help you debug your implementation.
# your class up here


if __name__ == "__main__":

	# from SOLUTION.mha import CustomMHA

	import torch
	import numpy as np

	D = 6
	H = 2
	mha = CustomMHA(D,H)

	# make some fixed weights
	# this just makes a really long 1-D np array and then reshapes it into the size we need
	tensor1 = torch.tensor(np.reshape(np.linspace(-2.0, 1.5, D*D*3), (D*3,D))).to(torch.float32)
	tensor2 = torch.tensor(np.reshape(np.linspace(-1.0, 2.0, D*D), (D,D))).to(torch.float32)

	# copy these into our MHA weights, so we don't need to worry about random initializations for testing
	mha.W_qkv.data = tensor1
	mha.W_o.data = tensor2

	# make an input tensor
	B = 2
	S = 3
	x = torch.tensor(np.reshape(np.linspace(-1.0, 0.5, B*S*D), (B,S,D))).to(torch.float32)

	# run
	y1 = mha(x)
	print(y1.shape)
	print(y1)

	'''
	Should print out:

	torch.Size([2, 3, 6])
	tensor([[[ 17.2176,   5.5439,  -6.1297, -17.8034, -29.4771, -41.1508],
         [ 17.4543,   5.5927,  -6.2688, -18.1304, -29.9920, -41.8536],
         [ 17.6900,   5.6398,  -6.4105, -18.4607, -30.5110, -42.5612]],

        [[ -1.3639,  -0.1192,   1.1256,   2.3703,   3.6151,   4.8598],
         [ -5.5731,  -1.9685,   1.6361,   5.2407,   8.8453,  12.4499],
         [ -5.6875,  -2.0716,   1.5444,   5.1603,   8.7762,  12.3922]]],
       grad_fn=<UnsafeViewBackward0>)

	'''

