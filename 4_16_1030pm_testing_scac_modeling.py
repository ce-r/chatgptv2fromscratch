# -*- coding: utf-8 -*-
"""4_16_1030pm_TESTING_scac_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NG-qznQPpYTPvubkd_leO6Ut2MpSM40T
"""

# !huggingface-cli login
# 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

!pip install pypdf
!pip install SQLAlchemy==1.4.49

!pip install fitz
!pip install tools
!pip install bs4

!pip install faiss-cpu
!pip install farm-haystack[sentence-transformers]

!pip install pdfplumber

!pip install transformers datasets
!pip install -U scikit-learn

!pip uninstall -y pydantic
!pip install pydantic==1.10.13

from google.colab import drive
drive.mount('/content/drive')

import os
from pathlib import Path
from bs4 import BeautifulSoup
from pypdf import PdfReader
import json
from haystack.document_stores import FAISSDocumentStore
from haystack.nodes import EmbeddingRetriever, PreProcessor
from haystack.schema import Document
from typing import List, Optional
import logging
import warnings
import pickle

import sys
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from concurrent.futures import ThreadPoolExecutor, TimeoutError
import faiss
import pickle
from tqdm import tqdm
from copy import deepcopy
# from accelerate import Accelerator

import time
import pdfplumber
import gc
from transformers import T5Tokenizer, T5ForConditionalGeneration

"""# Warning Suppression"""

logging.getLogger("pdfminer").setLevel(logging.ERROR)

"""# File paths and extensions for document, model processing/loading"""

DOCKETS_DIR = Path("/content/drive/MyDrive/dockets_test1")
CSV_PATH = "scac-filings4db.csv"
FAISS_INDEX_PATH = "full_faiss_index"
LOG_FILE = "generation_log.txt"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
EXTENSIONS = [".txt", ".pdf", ".html"]

"""# Load Huggingface model"""

model_name = "google/flan-t5-large"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name,
                                                   torch_dtype=torch.float16,
                                                   device_map="auto")

# Write log files for prompt and input with case info titles
def save_prompt_and_output(prompt, response, task, case_id, file_name, idx):
  with open(f"input_{task}_{case_id}_{file_name}_chunk{idx}.txt", "w", encoding="utf-8") as f:
    f.write(prompt)
  with open(f"output_{task}_{case_id}_{file_name}_chunk{idx}.txt", "w", encoding="utf-8") as f:
    f.write(response)

# Text extraction for certain file types including pdf, txt and html
def extract_text(file_path: Path):
  if file_path.suffix == ".pdf":
    with pdfplumber.open(file_path) as pdf:
      return "\n".join(page.extract_text() or "" for page in pdf.pages)
  elif file_path.suffix == ".html":
    return BeautifulSoup(file_path.read_text(encoding="utf-8", errors="ignore"), "html.parser").get_text()
  elif file_path.suffix == ".txt":
    return file_path.read_text(encoding="utf-8", errors="ignore")
  return ""

# Preprocessing of text to remove excessive whitespace and return line separated string
def preprocess_text(text):
  text = ' '.join(text.split())
  lines = [line for line in text.split('\n') if len(line.strip()) > 30]
  return '\n'.join(lines)

# Hand crafted prompts for value extraction of two features, violations and settlement_amount
def build_prompt(task, text):
  examples = {"violations": (
                "From the legal document below, explicitly identify all cited SEC violation references, "
                "specifying exact sections and rules (e.g., 'Section 10(b) of the Securities Exchange Act of 1934, Rule 10b-5'). "
                "If no violations are cited, respond strictly with 'None'.\n\n"
                f"Document:\n{text}\n\n"
                "Violations:"),

              "settlement_amount": (
                "Extract explicitly stated monetary amounts associated with a settlement, judgment, or resolution.\n"
                "Respond ONLY with the dollar value mentioned (e.g., '$250 million', '$250000'). If none is present, respond 'None'.\n\n"
                "Example 1:\n"
                "Text: The case settled for $250 million in July.\n"
                "Amount: $250 million\n\n"
                "Example 2:\n"
                "Text: Judgment was entered with $250000 for the plaintiffs.\n"
                "Amount: $250000\n\n"
                "Now for this document:\n"
                f"Document:\n{text}\n\n"
                "Amount:")}
  return examples[task]

# Chunking with concatenation of prompt and input text,
# 50 token overlap from one chunk to the next, 10 token
# buffer to avoid exceeting model's max token limit
def build_chunks(task, text, max_tokens=512, overlap=50):
  base_prompt = build_prompt(task, "")
  base_tokens = tokenizer.encode(base_prompt, return_tensors="pt")[0]
  base_length = len(base_tokens)

  text_tokens = tokenizer.encode(text, return_tensors="pt")[0]
  text_max_length = max_tokens - base_length - 10  # 10 token buffer for safety

  chunks = []
  start = 0
  while start < len(text_tokens):
    end = min(start + text_max_length, len(text_tokens))
    chunk = text_tokens[start:end]
    combined = torch.cat([base_tokens, chunk])
    chunks.append(tokenizer.decode(combined, skip_special_tokens=True))
    start += text_max_length - overlap

  return chunks

# Generation with cleanup, timeout, logging
# Running the generator on an explicit thread
# allows control of the timeout mechanism
# Allows model generation to continue in spite of timeout
def safe_generate(prompt, tokenizer, model, max_input_tokens=512, timeout_sec=60):
  def _generate():
    try:
      inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_input_tokens).to(model.device)
      with torch.no_grad():
        output = model.generate(**inputs,
                                max_new_tokens=100,
                                num_beams=5,
                                do_sample=False,  # deterministic
                                early_stopping=True,
                                no_repeat_ngram_size=2)
        return tokenizer.decode(output[0], skip_special_tokens=True).strip()
    except Exception as e:
      return f"[ERROR] {str(e)}"

  with ThreadPoolExecutor(max_workers=1) as executor:
    future = executor.submit(_generate)
    try:
      return future.result(timeout=timeout_sec)
    except TimeoutError:
      print("[TIMEOUT] Generation took too long.")
      return "[TIMEOUT]"

# Same as safe_generate with batching to improve processing
# speed but falls back on safe_generate if method times out
def safe_batch_generate(prompts, tokenizer, model, max_input_tokens=512, timeout_sec=60):
  def _generate_batch():
    try:
      inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=max_input_tokens)
      inputs = {k: v.to(model.device) for k, v in inputs.items()}
      with torch.no_grad():
        outputs = model.generate(**inputs,
                                 max_new_tokens=100,
                                 num_beams=5,
                                 do_sample=False,
                                 early_stopping=True,
                                 no_repeat_ngram_size=2)
      return [tokenizer.decode(output, skip_special_tokens=True).strip() for output in outputs]
    except Exception as e:
      return [f"[ERROR] {str(e)}"] * len(prompts)

  with ThreadPoolExecutor(max_workers=1) as executor:
    future = executor.submit(_generate_batch)
    try:
      return future.result(timeout=timeout_sec)
    except TimeoutError:
      print("[TIMEOUT] Batched generation took too long. Falling back to safe_generate().")
      return [safe_generate(p, tokenizer, model, max_input_tokens=max_input_tokens) for p in prompts]

# Test for quality/acceptability of model response
def is_quality_response(response):
  return (response.strip().lower() != "none"
          and len(response.strip()) > 1
          and not response.lower().startswith("case no.")
          and "court" not in response.lower()
          and ("$" in response if "settlement" in response else True))

"""# Transformer only model"""

def transformer_pipeline(df, tokenizer, model, DOCKETS_DIR, output_csv):
  # Initialize counters and logging variables
  processed_cases = 0
  successful_extractions = 0
  start_time = time.time()

  # Clear memory before starting
  torch.cuda.empty_cache()
  gc.collect()

  # Loop over dockets directory,
  # extract text from documents,
  # match case_id from csv file
  # with case_id generated from
  # dir walk-through
  for ticker_dir in tqdm(list(DOCKETS_DIR.iterdir()), desc="Processing Ticker Folders"):
    if not ticker_dir.is_dir():
      continue

    ticker = ticker_dir.name

    for filing_dir in ticker_dir.iterdir():
      if not filing_dir.is_dir():
        continue

      filing_date = filing_dir.name
      case_id = f"{ticker}_{filing_date}"

      if case_id not in df["case_id"].values:
        continue

      processed_cases += 1
      print(f"\n[PROCESSING CASE] {case_id}")

      # dicts for final output dataframe,
      # best_outputs and for end of task
      # boolean flag, field_hits
      best_outputs = {"violations": None, "settlement_amount": None}
      field_hits = {"violations": False, "settlement_amount": False}

      # dir stucture: dockets->ticker->filing_date->doc_title->file(pdf/txt/html)
      for doc_dir in filing_dir.iterdir():
        if not doc_dir.is_dir():
          continue
        files = list(doc_dir.glob("*"))
        if not files:
          continue

        file = files[0]  # Process first file in directory

        try:
          text = extract_text(file)
          text = preprocess_text(text)
          if not text.strip():
            continue

          print(f"[PROCESSING] {case_id} -> Document: {file.name}")

          # after collecting text, iterate over two tasks
          # if task is not true proceed to generate text
          # from llm to populate best_outputs dictionary
          for task in ["violations", "settlement_amount"]:
            if field_hits[task]:
              continue

            # Create text chunks and batch them for parallel processing
            chunks = build_chunks(task, text)
            # Generate model response
            responses = safe_batch_generate(chunks, tokenizer, model)

            # chunk/prompt: prompt + input text
            for idx, (prompt, response) in enumerate(zip(chunks, responses)):
              try:
                # Validate input length
                token_len = len(tokenizer(prompt, return_tensors="pt").input_ids[0])
                if token_len > 512: # 2048
                  # print(f"[SKIP] Chunk {idx} too long for model context window.")
                  continue

                print(f"[DEBUG] chunk {idx} -> token count: {token_len}")
                # Write input/output to log file
                save_prompt_and_output(prompt, response, task, case_id, file.name, idx)

                if is_quality_response(response):
                  best_outputs[task] = response
                  field_hits[task] = True
                  print(f"[SUCCESS] Found {task} for {case_id}: {response}")
                  break

                # Save prompt for debugging
                with open(f"input_{task}_{case_id}_{file.name}_chunk{idx}.txt", "w", encoding="utf-8") as f:
                  f.write(prompt)

                with open(f"output_{task}_{case_id}_{file.name}_chunk{idx}.txt", "w", encoding="utf-8") as f:
                  f.write(response)

              except Exception as e:
                response = "None"

              finally:
                # Clear cache
                torch.cuda.empty_cache()
                gc.collect()

        except Exception as e:
          print(f"[ERROR] Processing failed for {file.name}: {str(e)}")
          continue

      # Update dataframe if we find any results
      if any(best_outputs.values()):
        idx = df[df["case_id"] == case_id].index[0]
        for task in ["violations", "settlement_amount"]:
          if best_outputs[task]:
            df.at[idx, task] = best_outputs[task]
            successful_extractions += 1

        # Save results in csv file
        df.to_csv(output_csv, index=False)
        print(f"[PROGRESS] Saved results for {case_id}")

  # Save stats, runtime and counts of successful model responses
  total_time = time.time() - start_time
  df.to_csv(output_csv, index=False)

  print("\n[SUMMARY]")
  print(f"  Processed cases: {processed_cases}")
  print(f"  Successful extractions: {successful_extractions}")
  print(f"  Total runtime: {total_time:.2f} seconds")
  print(f"[DONE] Results saved to {output_csv}")

  return df

"""# ----------------------------------------------------------
# RAG store

## Note: We only need to worry about how the document store is created insofar as it aligns with how the retriever embeds the query.
"""

# Document loading of file types pdf, txt and html
def load_document(fpath: str):
  ext = os.path.splitext(fpath)[1].lower()
  try:
    if ext == ".txt":
      with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()
    elif ext == ".pdf":
      reader = PdfReader(fpath)
      return "\n".join(page.extract_text() or "" for page in reader.pages)
    elif ext == ".html":
      with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
        soup = BeautifulSoup(f, "html.parser")
        return soup.get_text(separator="\n")
  except Exception as e:
    print(f"[WARN] Failed to load {fpath}: {e}")
    return None

# Google drive root directory of docket folders
DOC_ROOT = "/content/drive/MyDrive/dockets_test1"
# Output path for FAISS index and docs
FAISS_INDEX_PATH = "/content/full_faiss_index"
# Transformer model to use for embeddings
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
# Accept file types to load
EXTENSIONS = [".txt", ".pdf", ".html"]

"""# Load/Build RAG store"""

def create_rag_components(index_dir="full_faiss_index",
                          doc_root="/content/drive/MyDrive/dockets_test1",
                          embedding_model="sentence-transformers/all-MiniLM-L6-v2"):

  # Doc store paths
  docs_path = os.path.join(index_dir, "docs.pkl")
  faiss_index_path = os.path.join(index_dir, "faiss_index.faiss")
  faiss_config_path = os.path.join(index_dir, "faiss_config.json")
  index_map_path = os.path.join(index_dir, "faiss_index.json")

  # If all files exist, load from disk
  if all(os.path.exists(p) for p in [docs_path, faiss_index_path, faiss_config_path, index_map_path]):
    print("[INFO] Loading FAISS index and retriever...")

    # Load documents
    with open(docs_path, "rb") as f:
      documents = pickle.load(f)

    # Load index map
    with open(index_map_path, "r") as f:
      index_map = json.load(f)

    # Load FAISS index from disk
    document_store = FAISSDocumentStore.load(index_path=faiss_index_path,
                                             config_path=faiss_config_path)

    retriever = EmbeddingRetriever(document_store=document_store,
                                   embedding_model=embedding_model,
                                   use_gpu=torch.cuda.is_available())

    index = faiss.read_index(faiss_index_path)

    print(f"[INFO] Loaded {len(documents)} documents and {len(index_map)} FAISS index entries.")
    return documents, index, index_map, retriever
  # Build doc store from scratch
  else:
    print("[INFO] FAISS components not found. Building from scratch...")
    os.makedirs(index_dir, exist_ok=True)
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    doc_store = FAISSDocumentStore(embedding_dim=384,
                                   faiss_index_factory_str="Flat",
                                   sql_url="sqlite:///rag_legal.db")

    retriever = EmbeddingRetriever(document_store=doc_store,
                                   embedding_model=embedding_model,
                                   model_format="sentence_transformers",
                                   use_gpu=torch.cuda.is_available(),
                                   progress_bar=True)
    # Walk dockets dir, collect and load docs with
    # meta data, chunk, embed docs and populate rag store
    # Note rag embeddings are used for retrieval in a similarity search
    documents = []
    for root, _, files in os.walk(doc_root):
      for fname in files:
        if not any(fname.endswith(ext) for ext in EXTENSIONS):
          continue
        fpath = os.path.join(root, fname)
        path_parts = Path(fpath).relative_to(doc_root).parts
        if len(path_parts) < 3:
          continue
        ticker, filing_date, doc_title = path_parts[:3]
        case_id = f"{ticker}_{filing_date}"
        text = load_document(fpath)
        if not text or not text.strip():
          continue
        documents.append(
            Document(content=text, meta={
                "case_id": case_id, "ticker": ticker, "filing_date": filing_date,
                "doc_title": doc_title, "source": fname, "path": fpath}))

    logger.info(f"Loaded {len(documents)} documents.")

    preprocessor = PreProcessor(clean_empty_lines=True,
                                clean_whitespace=True,
                                clean_header_footer=True,
                                split_length=150,
                                split_overlap=30,
                                split_respect_sentence_boundary=True,
                                language="en")

    chunks = preprocessor.process(documents)
    logger.info(f"Created {len(chunks)} chunks.")

    doc_store.write_documents(chunks)
    doc_store.update_embeddings(retriever)
    logger.info(f"Stored {doc_store.get_document_count()} docs, {doc_store.get_embedding_count()} embeddings.")

    docs_all = doc_store.get_all_documents()
    index_map = {}
    for idx, doc in enumerate(docs_all):
      cid = doc.meta.get("case_id")
      if cid:
        index_map.setdefault(cid, []).append(idx)

    with open(docs_path, "wb") as f:
      pickle.dump(docs_all, f)
    with open(index_map_path, "w") as f:
      json.dump(index_map, f)

    doc_store.save(index_path=faiss_index_path, config_path=faiss_config_path)
    index = faiss.read_index(faiss_index_path)

    # Return all pertinent rag components for later use
    print(f"[DONE] Saved FAISS index, docs, and map to '{index_dir}'")
    return docs_all, index, index_map, retriever

"""# RAG + Transformer"""

# Accepts indices and documents as arguments
# in order to access the rag store
def get_doc_texts_from_indices(indices, documents):
  texts = []
  for idx in indices:
    if 0 <= idx < len(documents):
      doc = documents[idx]
      text = getattr(doc, "content", "") if hasattr(doc, "content") else doc.get("content", "")
      if text:
        texts.append(text)
  return texts

"""# EmbeddingRetriever does not have an .encode() method, only SentenceTransformer models do.

# Runs the Rag + Transformer model
# The model used is exactly the same as that contained in the Transformer only version.
# This is deliberately done for comparison between the pipelines.
# The RAG store should improve speed, at the very least.
# The improved speed is only observed, if the RAG store has already been generated on a previous run
"""

def rag_transformer_inference(df,
                              tokenizer,
                              model,
                              encoder,
                              documents,
                              faiss_index,
                              index_map,
                              top_k=3,
                              max_length=512,
                              output_csv="structured_output_rag.csv"):

  num_violations_filled = 0
  num_settlements_filled = 0
  start_time = time.time()

  for i, row in tqdm(df.iterrows(), total=len(df), desc="RAG Inference"):
    case_id = row["case_id"]
    if pd.notna(row["violations"]) and pd.notna(row["settlement_amount"]):
      continue

    if case_id not in index_map:
      continue

    retrieved_chunks = get_doc_texts_from_indices(index_map[case_id], documents)
    if not retrieved_chunks:
      continue

    # Join all chunks for a full document
    full_text = preprocess_text(" ".join(retrieved_chunks))

    best_outputs = {"violations": None, "settlement_amount": None}
    field_hits = {"violations": False, "settlement_amount": False}

    for task in ["violations", "settlement_amount"]:
      if field_hits[task]:
        continue

      prompt_chunks = build_chunks(task, full_text)
      responses = safe_batch_generate(prompt_chunks, tokenizer, model)

      for idx, (prompt, response) in enumerate(zip(prompt_chunks, responses)):
        try:
          if "[ERROR] CUDA out of memory" in response:
            print(f"[OOM DETECTED] Falling back to safe_generate on chunk {idx}")
            response = safe_generate(prompt, tokenizer, model, max_input_tokens=max_length)

          token_len = len(tokenizer(prompt, return_tensors="pt").input_ids[0])
          if token_len > 512:
            continue

          print(f"[DEBUG] Chunk {idx} -> token count: {token_len}")
          save_prompt_and_output(prompt, response, task, case_id, f"RAG_logs.txt", idx)

          if is_quality_response(response):
            best_outputs[task] = response
            field_hits[task] = True
            print(f"[SUCCESS][RAG] {task} for {case_id}: {response}")
            break
        except Exception as e:
          # print(f"[ERROR] Saving or processing response: {e}")
          response = "None"
        finally:
          torch.cuda.empty_cache()
          gc.collect()

    if any(best_outputs.values()):
      idx = df[df["case_id"] == case_id].index[0]
      for task in ["violations", "settlement_amount"]:
        if best_outputs[task]:
          df.at[idx, task] = best_outputs[task]
          if task == "violations":
            num_violations_filled += 1
          elif task == "settlement_amount":
            num_settlements_filled += 1

  total_time = round(time.time() - start_time, 2)
  df.to_csv(output_csv, index=False)

  print("\n[RAG + Transformer]")
  print(f"  Violations filled: {num_violations_filled}/{len(df)}")
  print(f"  Settlements filled: {num_settlements_filled}/{len(df)}")
  print(f"  Runtime: {total_time:.2f} seconds")
  print(f"[DONE] RAG output saved -> {output_csv}")

"""# Summary table of outputs"""

def summarize(df, label, elapsed_time):
  v_filled = df["violations"].notna().sum()
  s_filled = df["settlement_amount"].notna().sum()
  total = len(df)

  print(f"\n[{label}]")
  print(f"  Violations filled: {v_filled}/{total}")
  print(f"  Settlements filled: {s_filled}/{total}")
  print(f"  Runtime: {elapsed_time:.2f} seconds")
  return {"method": label,
          "violations_filled": v_filled,
          "settlement_filled": s_filled,
          "total": total,
          "runtime_sec": round(elapsed_time, 2)}

"""# Method to compare Transformer-Only and RAG+Transformer pipeline"""

# Compare Transformer w/ RAG Pipelines
def compare_pipelines(df_original,
                                 DOCKETS_DIR,
                                 tokenizer,
                                 model,
                                 retriever,
                                 encoder,
                                 rag_documents,
                                 faiss_index,
                                 index_map,
                                 output_transformer="structured_output_transformer.csv",
                                 output_rag="structured_output_rag.csv"):

  for col in ["violations", "settlement_amount"]:
    if col not in df_original.columns:
      df_original[col] = None

  # Make copies to avoid modifying the original dataframe
  df_transformer = deepcopy(df_original)
  df_rag = deepcopy(df_original)

  # Run Transformer-Only pipeline
  print("\n[T-INFO] Running Transformer-Only Pipeline...")
  start_t = time.time()
  transformer_pipeline(df_transformer, tokenizer, model, DOCKETS_DIR, output_transformer)
  time_transformer = time.time() - start_t

  # Run RAG + Transformer pipeline
  print("\n[RT-INFO] Running RAG + Transformer Pipeline...")
  start_r = time.time()

  # Must use same encoder used for FAISS document embeddings
  rag_transformer_inference(df=df_rag,
                            tokenizer=tokenizer,
                            model=model,
                            encoder=encoder,
                            documents=rag_documents,
                            faiss_index=faiss_index,
                            index_map=index_map,
                            output_csv=output_rag)

  time_rag = time.time() - start_r

  # Collect results
  summary = [summarize(df_transformer, "Transformer-Only", time_transformer),
             summarize(df_rag, "RAG+Transformer", time_rag)]

  return pd.DataFrame(summary)

"""# Remove doc store files when rerunning scripts"""

# !rm -rf full_faiss_index
# !rm -f rag_legal.db

"""# Run both pipelines with comparison output found in pipeline_comparison_results.csv"""

if __name__ == "__main__":
  print("[INFO] Starting SCAC Modeling Pipeline...")
  documents, index, index_map, retriever = create_rag_components()
  encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
  df = pd.read_csv(CSV_PATH) # scac-filings4db.csv

  comparison_df = compare_pipelines(df_original=df,
                                    DOCKETS_DIR=DOCKETS_DIR,
                                    tokenizer=tokenizer,
                                    model=model,
                                    retriever=retriever,
                                    encoder=encoder,
                                    rag_documents=documents,
                                    faiss_index=index,
                                    index_map=index_map)

  comparison_df.to_csv("pipeline_comparison_results.csv", index=False)
  print("[DONE] Comparison results saved to pipeline_comparison_results.csv")

