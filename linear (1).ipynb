{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Questions\n",
        "Please write a short response (4-5 sentences / short paragraph) to each of the following questions / the following question. Your responses will be graded for accuracy, critical thinking, and clarity. You may use any common word processing or text format. Please upload your answers by the due date.\n",
        "\n",
        "\n",
        "\n",
        "Question 1: Explain the concept of a tensor, and the differences in PyTorch between a generic tensor, a parameter, and a gradient.\n",
        "\n",
        "The tensor is a data structure and/or object that is specifically designed for matrix storage and operations. The parameter is a torch object designed to learn from the model, in the same way coefficients of a regression model are estimated. The gradient is dealt with through arguments or various methods. When we instantiate a tensor object, we can use an argument of requires_grad=True to track gradients during training. We can compute gradients with .backward() or access them with .grad. We can use zero_grad() to clear the gradients during batch/epoch training.  "
      ],
      "metadata": {
        "id": "lOKSRc87ssc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Osfp3JAO8qTF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "'''\n",
        "Complete this class by instantiating parameters called \"self.weight\" and \"self.bias\", and\n",
        "use them to complete the forward() method. You do not need to worry about backpropogation.\n",
        "'''\n",
        "class CustomLinear(torch.nn.Module):\n",
        "# input_size is the number of features in input\n",
        "# output_size is the number of features in the output\n",
        "# shape of biases is output_size\n",
        "\n",
        "\tdef __init__(self, input_size, output_size):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# generate tensor of random vals from std normal distr with shape\n",
        "    # output_size, input_size and scale vals by 0.1, to avoid beginning\n",
        "    # w/ large weights potentially leading to unstable training\n",
        "    initial_weights = 0.1*torch.randn(output_size, input_size)\n",
        "    # initialize learnable weight param\n",
        "    self.weight = torch.nn.Parameter(initial_weights)\n",
        "    # initialize learnable bias param w/ shape (output_size,) and zero vals\n",
        "    self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t'''\n",
        "\t\tx is a tensor, which contains a batch of vectors, size (B, input_size).\n",
        "\t\tThis should return a tensor of size (B, output_size).\n",
        "\t\t'''\n",
        "    # since the weights and biases in a layer are shared across all samples in\n",
        "    # a batch.. we don't need to explicitly handle batch size because it is\n",
        "    # handled automatically by pytorch broadcasting and multiplication rules\n",
        "    # the input tensor will have shape (B, input_size) and\n",
        "    # output tensor (B, output_size)\n",
        "\t\treturn x @ self.weight.T + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7LFVoTCd7-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}