# -*- coding: utf-8 -*-
"""chatgpt_module7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BCLmjV5fyr90xBh0Wv_RVvjdIC2xCVrI
"""

import torch
import numpy as np

from collections import Counter

# sampler.py

'''
Class implementing a sampler for inference on a model. Given the raw logits from
an LLM model, this will sample the next token id.
'''
class Sampler:
  def __init__(self, top_k=None, top_p=None, frequency_penalty=1.0, presence_penalty=1.0):
    '''
    param top_k : (None or int)
      If specified, only the top k logits should be used during sampling
      If this is specified, top_p should be None

    param top_p : (None or int)
      If specified, only the logits representing the probability mass p should be used during sampling.
      Or, if the top token has mass greater than p, the top token is returned.
      If this is specified, top_k should be None

    If top_k and top_p are both None, sample from the whole distribution (same as top_p=1.0)

    param frequency_penalty : (float)
      A penalty applied to tokens that have previously occured in the sequence. Along with
      presence_penalty, this adjusts the per-token softmax temperature.
      A penalty of 1.0 indicates no change from normal softmax.

    param presence_penalty : (float)
      A penalty applied to tokens ((( IF ))) they have previously occured in the sequence. Along with
      frequency_penalty, this adjusts the per-token softmax temperature.
      A penalty of 1.0 indicates no change from normal softmax.
    '''
		# TODO
    if top_k is not None and top_p is not None:
      raise ValueError("top_k and top_p cannot both be specified")

    self.top_k = top_k
    self.top_p = top_p
    self.frequency_penalty = frequency_penalty
    self.presence_penalty = presence_penalty

  def sample_token(self, raw_unsorted_logits, previous_token_ids):
    '''
    param: raw_unsorted_logits (float numpy array)
      A one dimensional list of logits representing an unnormalized distribution over next tokens
      These are "unsorted" in the sense that their order aligns with vocabulary order, not with probability.

    param: previous_token_ids (int numpy array)
      A one dimensional list of ids representing the previous tokens, for calculating repetition penalties.

    returns: a single token id (integer), sampled according to the specified sampling parameters
    '''

    # TODO

    # very rough outline:
    # make temperature=1.0 for each vocabulary option
    # adjust temps as needed with penalties
    # logits = logits - np.min(logits) to make sure all are positive
    # apply temps & softmax, then sort
    # find either the top-p or top-k cutoff
    # renormalize this portion by simply dividing by the sum
    # sample from our final distribution

    vocab_size = raw_unsorted_logits.shape[0]
    # k = np.ones(vocab_size) # initialize temperature/s

    # calculate penalities
    if len(previous_token_ids) > 0:
      uniq_tokens, counts = np.unique(previous_token_ids, return_counts=True)
      uniq_tokens_mask = uniq_tokens < vocab_size # bool for tokens in vocab
      uniq_tokens = uniq_tokens[uniq_tokens_mask] # filter uniq_tokens
      counts = counts[uniq_tokens_mask] # filter counts
      # k[uniq_tokens] += counts * (self.frequency_penalty - 1) # compute frequency penalty
      # k[uniq_tokens] += (self.presence_penalty - 1) # compute presence penalty

      # applying penalties directly to logits improves performance on example_use_with_gpt2.py
      raw_unsorted_logits[uniq_tokens] -= (self.frequency_penalty - 1) * counts # compute frequency penalty
      raw_unsorted_logits[uniq_tokens] -= (self.presence_penalty - 1) * (counts > 0) # compute presence penalty
                                                                                     # to tokens present at least once

    # new_logits = raw_unsorted_logits/k # logits adjusted by penalties
    new_logits = raw_unsorted_logits - np.max(raw_unsorted_logits) # for softmax stability
    new_logits = np.clip(new_logits, a_min=None, a_max=np.percentile(new_logits, 99)) # clip logits at 99th percentile
                                                                                      # to prevent token domination
    # apply softmax
    exp_logits = np.exp(new_logits)
    probs = exp_logits/np.sum(exp_logits)

    if self.top_k is not None:
      top_k_idxs = np.argpartition(probs, -self.top_k)[-self.top_k:] # select top k idxs
      top_k_idxs = top_k_idxs[np.argsort(probs[top_k_idxs])][::-1] # sort top k indices
      probs = probs[top_k_idxs] # filter for top k idxs
      probs /= probs.sum() # normalize probs
      token_sample = np.random.choice(top_k_idxs, p=probs) # sample

    elif self.top_p is not None:
      sorted_idxs = np.argsort(probs)[::-1] # sort probs by prob return idxs, desc
      sorted_probs = probs[sorted_idxs] # sort probs by sorted idxs
      cumulative_probs = np.cumsum(sorted_probs) # cumulative probability of correctly
                                                 # sorted probs on which to threshold
                                                 # smallset subset of probs
                                                 # cum prob >= top_p
      mask = (cumulative_probs - sorted_probs) < self.top_p # filtering for threshold distribution
      top_p_idxs = sorted_idxs[:np.sum(mask)] # get idxs for select probs
      probs = probs[top_p_idxs] # get probs from select distribution
      probs /= probs.sum() # normalize probs
      token_sample = np.random.choice(top_p_idxs, p=probs) # sample

    else:
      token_sample = np.random.choice(len(probs), p=probs) # sample from entire distribution

    return token_sample

  # an alternative way to call sample_token(), for convenience
  def __call__(self, raw_unsorted_logits, previous_token_ids):
    return self.sample_token(raw_unsorted_logits, previous_token_ids)

if __name__ == "__main__":

  # example of using this with dummy data, keeping everything in token ids
  sampler = Sampler(top_p=0.8, frequency_penalty=1.1, presence_penalty=1.1)
  sequence = [1,2,3,4,5]

  for i in range(10):
    # fake logits for a vocab of size 500
    logits = np.random.randn(500)

    # get next token in sequence
    next_token = sampler(logits, sequence)
    sequence.append(next_token)

  print(sequence)

!pwd

# import numpy as np
# import torch
from tqdm import tqdm
# from sampler import Sampler

# from module 6:
from gpt import GPTModel
from hftokenizer import HFTokenizer
# import train_model

# example_use_with_our_trained_model.py

'''
Example of using Sampler with our own GPT model trained on wikipedia data.
Just included for fun.
'''

# make sampling algo
samp = Sampler(top_p=0.8, frequency_penalty=1.1, presence_penalty=1.1)

# load our model
# model = GPTModel(d_model=512, n_heads=16, layers=8, vocab_size=10000, max_seq_len=512)
model = GPTModel(d_model=512, n_heads=16, layers=8, vocab_size=10000, max_seq_len=256)
model.load_state_dict(torch.load("./model_weights.pt"))
model.eval()

# make tokenizer
tokenizer = HFTokenizer()
tokenizer.load()

# =================================================================

# some text
intial_text = "Steve Jobs was"
token_ids = tokenizer.encode(intial_text)
token_ids = torch.tensor([token_ids])
print(token_ids)

# generate N more tokens. We are not using kv cache or anything smart.
# This may be pretty slow.
for i in tqdm(range(100)):

	# pass tokens through the model to get logits
	output = model(token_ids)[0,-1,:]

	# sample from the logits
	token_ids_np = token_ids.data.cpu().numpy()
	tok = samp(output.data.cpu().numpy(), token_ids_np)

	# add the resulting token id to our list
	token_ids_np = np.append(token_ids_np, tok)
	token_ids = torch.from_numpy(token_ids_np)

	# add back a batch size of 1
	token_ids = token_ids[None,:]

	# if we generated a stop token, stop!
	if tok == tokenizer.tokenizer.eos_token_id:
		break

token_ids = token_ids.data.cpu().numpy()[0]

# print out resulting ids
print(token_ids)

# print out the decoded text
print(tokenizer.decode(token_ids))

# example_use_with_gpt2.py

import numpy as np
import torch
from tqdm import tqdm

'''
This script uses the Sampler class to sample text from GPT2-small.

You do not need to utilize this script for the assignment, but it may be
helpful or informative to see your Sampler applied to a real model.

Note this will download about 550MB of parameter data so you can run gpt2.
'''

from transformers import AutoTokenizer, AutoModelForCausalLM
# from sampler import Sampler

samp = Sampler(top_p=0.8, frequency_penalty=1.1, presence_penalty=1.1)

# download gpt2 and the associated tokenizer
# you can try other sizes of gpt2 if you want, i.e. "gpt2-xl"
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
model.eval()

# some text
intial_text = "Thomas Jefferson was the"
token_ids = tokenizer.encode(intial_text, return_tensors='pt')[0]
print(token_ids)

# generate N more tokens. We are not using kv cache or anything smart.
# This may be pretty slow.
for i in tqdm(range(50)):

  # pass tokens through the model to get logits
  output = model(token_ids)["logits"][-1,:]

  # sample from the logits
  token_ids_np = token_ids.data.cpu().numpy()
  tok = samp(output.data.cpu().numpy(), token_ids_np)

  # add the resulting token id to our list
  token_ids_np = np.append(token_ids_np, tok)
  token_ids = torch.from_numpy(token_ids_np)


# print out resulting ids
print(token_ids)

# print out the decoded text
print(tokenizer.decode(token_ids))

